{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17401c99-aa92-4eb4-8aaa-f57bedabc3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\misss\\.cache\\kagglehub\\datasets\\kazanova\\sentiment140\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download the Sentiment140 dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "\n",
    "# Display the path to the downloaded dataset files\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1aa64c7-922c-4387-812d-e7cf588d2364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target                                               text\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1       0  is upset that he can't update his Facebook by ...\n",
      "2       0  @Kenichan I dived many times for the ball. Man...\n",
      "3       0    my whole body feels itchy and like its on fire \n",
      "4       0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\misss\\.cache\\kagglehub\\datasets\\kazanova\\sentiment140\\versions\\2\\training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "# Define column names for the dataset\n",
    "columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Load the dataset using the specified encoding\n",
    "data = pd.read_csv(file_path, encoding='latin-1', names=columns)\n",
    "\n",
    "# Keep only the 'target' and 'text' columns\n",
    "data = data[['target', 'text']]\n",
    "\n",
    "# Convert sentiment labels to binary (0 = negative, 1 = positive)\n",
    "data['target'] = data['target'].replace({4: 1})\n",
    "\n",
    "# Display the first rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e89eddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2757287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:/Users/misss/Formation/\n",
      "[nltk_data]     Projets/AnalyseSentiments/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to C:/Users/misss/Formation/\n",
      "[nltk_data]     Projets/AnalyseSentiments/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to C:/Users/misss/Formation/Pr\n",
      "[nltk_data]     ojets/AnalyseSentiments/nltk_data...\n",
      "[nltk_data] Error loading word_tokenize: Package 'word_tokenize' not\n",
      "[nltk_data]     found in index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords', download_dir='C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n",
    "nltk.download('punkt_tab', download_dir='C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n",
    "nltk.download('wordnet', download_dir='C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n",
    "nltk.download('word_tokenize', download_dir='C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n",
    "nltk.data.path.append('C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f892c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target                                               text  \\\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1       0  is upset that he can't update his Facebook by ...   \n",
      "2       0  @Kenichan I dived many times for the ball. Man...   \n",
      "3       0    my whole body feels itchy and like its on fire    \n",
      "4       0  @nationwideclass no, it's not behaving at all....   \n",
      "\n",
      "                                        text_cleaned  \\\n",
      "0    - awww, that's a bummer.  you shoulda got da...   \n",
      "1  is upset that he can't update his facebook by ...   \n",
      "2   i dived many times for the ball. managed to s...   \n",
      "3    my whole body feels itchy and like its on fire    \n",
      "4   no, it's not behaving at all. i'm mad. why am...   \n",
      "\n",
      "                                          text_tfidf  \\\n",
      "0  awww thats <OOV> bummer <OOV> shoulda got davi...   \n",
      "1  <OOV> upset <OOV> <OOV> cant update <OOV> face...   \n",
      "2  <OOV> dived many time <OOV> <OOV> ball managed...   \n",
      "3  <OOV> whole body feel itchy <OOV> like <OOV> <...   \n",
      "4  <OOV> <OOV> <OOV> behaving <OOV> <OOV> im mad ...   \n",
      "\n",
      "                             text_lem_with_stopwords  \\\n",
      "0  [awww, thats, <OOV>, bummer, <OOV>, shoulda, g...   \n",
      "1  [<OOV>, upset, <OOV>, <OOV>, cant, update, <OO...   \n",
      "2  [<OOV>, dived, many, time, <OOV>, <OOV>, ball,...   \n",
      "3  [<OOV>, whole, body, feel, itchy, <OOV>, like,...   \n",
      "4  [<OOV>, <OOV>, <OOV>, behaving, <OOV>, <OOV>, ...   \n",
      "\n",
      "                               text_lem_no_stopwords  \\\n",
      "0  [awww, thats, a, bummer, you, shoulda, got, da...   \n",
      "1  [is, upset, that, he, cant, update, his, faceb...   \n",
      "2  [i, dived, many, time, for, the, ball, managed...   \n",
      "3  [my, whole, body, feel, itchy, and, like, it, ...   \n",
      "4  [no, it, not, behaving, at, all, im, mad, why,...   \n",
      "\n",
      "                            text_stem_with_stopwords  \\\n",
      "0  [awww, that, <OOV>, bummer, <OOV>, shoulda, go...   \n",
      "1  [<OOV>, upset, <OOV>, <OOV>, cant, updat, <OOV...   \n",
      "2  [<OOV>, dive, mani, time, <OOV>, <OOV>, ball, ...   \n",
      "3  [<OOV>, whole, bodi, feel, itchi, <OOV>, like,...   \n",
      "4  [<OOV>, <OOV>, <OOV>, behav, <OOV>, <OOV>, im,...   \n",
      "\n",
      "                              text_stem_no_stopwords  \\\n",
      "0  [awww, that, bummer, shoulda, got, david, carr...   \n",
      "1  [upset, cant, updat, facebook, text, might, cr...   \n",
      "2  [dive, mani, time, ball, manag, save, 50, rest...   \n",
      "3             [whole, bodi, feel, itchi, like, fire]   \n",
      "4                        [behav, im, mad, cant, see]   \n",
      "\n",
      "                                           text_bert  \n",
      "0  <oov> <oov> - awww, that's a bummer.  you shou...  \n",
      "1  is upset that he can't update his facebook by ...  \n",
      "2  <oov> i dived many times for the ball. managed...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  <oov> no, it's not behaving at all. i'm mad. w...  \n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning: remove mentions, URLs and lowercase the text.\"\"\"\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def preprocess_tfidf(text):\n",
    "    \"\"\"Preprocess for TF-IDF: remove punctuation, lemmatize, handle stopwords and OOV.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) if word not in stop_words else OOV_TOKEN for word in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def preprocess_lem_with_stopwords(text):\n",
    "    \"\"\"Lemmatization with stopwords, OOV token replacement, returns token list.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) if word not in stop_words else OOV_TOKEN for word in tokens]\n",
    "        return tokens\n",
    "    return []\n",
    "\n",
    "\n",
    "def preprocess_lem_no_stopwords(text):\n",
    "    \"\"\"Lemmatization without stopwords, returns token list.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return tokens\n",
    "    return []\n",
    "\n",
    "\n",
    "def preprocess_stem_with_stopwords(text):\n",
    "    \"\"\"Stemming with stopwords and OOV handling, returns token list.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [stemmer.stem(word) if word not in stop_words else OOV_TOKEN for word in tokens]\n",
    "        return tokens\n",
    "    return []\n",
    "\n",
    "\n",
    "def preprocess_stem_no_stopwords(text):\n",
    "    \"\"\"Stemming without stopwords, returns token list.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "        return tokens\n",
    "    return []\n",
    "\n",
    "\n",
    "def preprocess_bert(text):\n",
    "    \"\"\"Minimal preprocessing for BERT: lowercase, replace mentions and URLs with OOV.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"@\\w+\", OOV_TOKEN, text)\n",
    "        text = re.sub(r\"http\\S+\", OOV_TOKEN, text)\n",
    "        return text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "data[\"text_cleaned\"] = data[\"text\"].apply(clean_text)\n",
    "data[\"text_tfidf\"] = data[\"text_cleaned\"].apply(preprocess_tfidf)\n",
    "data[\"text_lem_with_stopwords\"] = data[\"text_cleaned\"].apply(preprocess_lem_with_stopwords)\n",
    "data[\"text_lem_no_stopwords\"] = data[\"text_cleaned\"].apply(preprocess_lem_no_stopwords)\n",
    "data[\"text_stem_with_stopwords\"] = data[\"text_cleaned\"].apply(preprocess_stem_with_stopwords)\n",
    "data[\"text_stem_no_stopwords\"] = data[\"text_cleaned\"].apply(preprocess_stem_no_stopwords)\n",
    "data[\"text_bert\"] = data[\"text\"].apply(preprocess_bert)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac19944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned dataset to a CSV file\n",
    "data.to_csv('tweets_cleaned.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
