{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17401c99-aa92-4eb4-8aaa-f57bedabc3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\misss\\Formation\\Projets\\AnalyseSentiments\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\misss\\.cache\\kagglehub\\datasets\\kazanova\\sentiment140\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa64c7-922c-4387-812d-e7cf588d2364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target                                               text\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1       0  is upset that he can't update his Facebook by ...\n",
      "2       0  @Kenichan I dived many times for the ball. Man...\n",
      "3       0    my whole body feels itchy and like its on fire \n",
      "4       0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\misss\\.cache\\kagglehub\\datasets\\kazanova\\sentiment140\\versions\\2\\training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "data = pd.read_csv(file_path, encoding='latin-1', names=columns)\n",
    "\n",
    "data = data[['target', 'text']]\n",
    "\n",
    "# Convertir les sentiments en binaire (0 = négatif, 1 = positif)\n",
    "data['target'] = data['target'].replace({4: 1})\n",
    "\n",
    "\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89eddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2757287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:/Users/misss/Formation/\n",
      "[nltk_data]     Projets/AnalyseSentiments/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to C:/Users/misss/Formation/\n",
      "[nltk_data]     Projets/AnalyseSentiments/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to C:/Users/misss/Formation/Pr\n",
      "[nltk_data]     ojets/AnalyseSentiments/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading word_tokenize: Package 'word_tokenize' not\n",
      "[nltk_data]     found in index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords', download_dir='C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n",
    "nltk.download('punkt_tab', download_dir='C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n",
    "nltk.download('wordnet', download_dir='C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n",
    "nltk.download('word_tokenize', download_dir='C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n",
    "nltk.data.path.append('C:/Users/misss/Formation/Projets/AnalyseSentiments/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f892c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target                                               text  \\\n",
      "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1       0  is upset that he can't update his Facebook by ...   \n",
      "2       0  @Kenichan I dived many times for the ball. Man...   \n",
      "3       0    my whole body feels itchy and like its on fire    \n",
      "4       0  @nationwideclass no, it's not behaving at all....   \n",
      "\n",
      "                                        text_cleaned  \\\n",
      "0    - awww, that's a bummer.  you shoulda got da...   \n",
      "1  is upset that he can't update his facebook by ...   \n",
      "2   i dived many times for the ball. managed to s...   \n",
      "3    my whole body feels itchy and like its on fire    \n",
      "4   no, it's not behaving at all. i'm mad. why am...   \n",
      "\n",
      "                                          text_tfidf  \\\n",
      "0  awww thats bummer shoulda got david carr third...   \n",
      "1  upset cant update facebook texting might cry r...   \n",
      "2  dived many time ball managed save 50 rest go b...   \n",
      "3                    whole body feel itchy like fire   \n",
      "4                           behaving im mad cant see   \n",
      "\n",
      "                             text_lem_with_stopwords  \\\n",
      "0  [awww, thats, bummer, shoulda, got, david, car...   \n",
      "1  [upset, cant, update, facebook, texting, might...   \n",
      "2  [dived, many, time, ball, managed, save, 50, r...   \n",
      "3             [whole, body, feel, itchy, like, fire]   \n",
      "4                     [behaving, im, mad, cant, see]   \n",
      "\n",
      "                               text_lem_no_stopwords  \\\n",
      "0  [awww, thats, a, bummer, you, shoulda, got, da...   \n",
      "1  [is, upset, that, he, cant, update, his, faceb...   \n",
      "2  [i, dived, many, time, for, the, ball, managed...   \n",
      "3  [my, whole, body, feel, itchy, and, like, it, ...   \n",
      "4  [no, it, not, behaving, at, all, im, mad, why,...   \n",
      "\n",
      "                            text_stem_with_stopwords  \\\n",
      "0  [awww, that, a, bummer, you, shoulda, got, dav...   \n",
      "1  [is, upset, that, he, cant, updat, hi, faceboo...   \n",
      "2  [i, dive, mani, time, for, the, ball, manag, t...   \n",
      "3  [my, whole, bodi, feel, itchi, and, like, it, ...   \n",
      "4  [no, it, not, behav, at, all, im, mad, whi, am...   \n",
      "\n",
      "                              text_stem_no_stopwords  \\\n",
      "0  [awww, that, bummer, shoulda, got, david, carr...   \n",
      "1  [upset, cant, updat, facebook, text, might, cr...   \n",
      "2  [dive, mani, time, ball, manag, save, 50, rest...   \n",
      "3             [whole, bodi, feel, itchi, like, fire]   \n",
      "4                        [behav, im, mad, cant, see]   \n",
      "\n",
      "                                           text_bert  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "\n",
    "# Fonction de nettoyage général (suppression des mentions, URL et conversion en minuscule)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Supprimer les mentions\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Supprimer les URLs\n",
    "    text = text.lower()  # Convertir en minuscule\n",
    "    return text\n",
    "\n",
    "#  Nettoyage pour TF-IDF (lemmatisation et suppression des stopwords) avec gestion OOV\n",
    "def preprocess_tfidf(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Supprimer la ponctuation\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) if word not in stop_words else OOV_TOKEN for word in tokens]  \n",
    "        return \" \".join(tokens)  # Retourner sous forme de phrase entière\n",
    "    return \"\"\n",
    "\n",
    "#  Nettoyage pour Word2Vec / FastText (avec stopwords et OOV)\n",
    "def preprocess_lem_with_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) if word not in stop_words else OOV_TOKEN for word in tokens]\n",
    "        return tokens  # Garder la liste de tokens\n",
    "    return []\n",
    "\n",
    "#  Nettoyage pour Word2Vec / FastText (sans stopwords et avec OOV)\n",
    "def preprocess_lem_no_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]  \n",
    "        return tokens\n",
    "    return []\n",
    "\n",
    "#  Nettoyage pour Word Embeddings avec stemming et stopwords\n",
    "def preprocess_stem_with_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [stemmer.stem(word) if word not in stop_words else OOV_TOKEN for word in tokens]\n",
    "        return tokens\n",
    "    return []\n",
    "\n",
    "#  Nettoyage pour Word Embeddings avec stemming sans stopwords\n",
    "def preprocess_stem_no_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]  \n",
    "        return tokens\n",
    "    return []\n",
    "\n",
    "# Nettoyage pour BERT (on garde le texte brut mais on ajoute un OOV token explicite)\n",
    "def preprocess_bert(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r\"@\\w+\", OOV_TOKEN, text)  # Remplace les mentions\n",
    "        text = re.sub(r\"http\\S+\", OOV_TOKEN, text)  # Remplace les URLs\n",
    "        text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Appliquer les nettoyages et créer les nouvelles colonnes\n",
    "data[\"text_cleaned\"] = data[\"text\"].apply(clean_text)\n",
    "data[\"text_tfidf\"] = data[\"text_cleaned\"].apply(preprocess_tfidf)\n",
    "data[\"text_lem_with_stopwords\"] = data[\"text_cleaned\"].apply(preprocess_lem_with_stopwords)\n",
    "data[\"text_lem_no_stopwords\"] = data[\"text_cleaned\"].apply(preprocess_lem_no_stopwords)\n",
    "data[\"text_stem_with_stopwords\"] = data[\"text_cleaned\"].apply(preprocess_stem_with_stopwords)\n",
    "data[\"text_stem_no_stopwords\"] = data[\"text_cleaned\"].apply(preprocess_stem_no_stopwords)\n",
    "data[\"text_bert\"] = data[\"text\"].apply(preprocess_bert)\n",
    "\n",
    "# Affichage des premières lignes\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac19944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sauve les données nettoyées dans un CSV\n",
    "data.to_csv('tweets_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
