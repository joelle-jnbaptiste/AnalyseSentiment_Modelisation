{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "\n",
    "data = pd.read_csv('data/tweets_cleaned.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "target     int64\n",
      "text      object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "print(data['target'].unique())\n",
    "data['target'] = data['target'].astype(int)\n",
    "print(data.dtypes)\n",
    "\n",
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, run_id):\n",
    "    \"\"\"\n",
    "    Génère, affiche et enregistre une matrice de confusion.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Négatif\", \"Positif\"], yticklabels=[\"Négatif\", \"Positif\"])\n",
    "    plt.xlabel(\"Prédictions\")\n",
    "    plt.ylabel(\"Vraies valeurs\")\n",
    "    plt.title(f\"Matrice de Confusion - {model_name}\")\n",
    "\n",
    "    # Affichage dans le notebook\n",
    "    plt.show()\n",
    "\n",
    "    # Sauvegarde de l'image\n",
    "    filename = f\"confusion_matrix_{model_name}_run{run_id}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:10:24 INFO mlflow.tracking.fluent: Experiment with name 'TF-IDF + Logistic Regression' does not exist. Creating a new experiment.\n",
      "2025/01/06 17:10:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=5000, ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:11:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=5000, ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:12:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=5000, ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:12:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=10000, ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:13:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=10000, ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:14:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=10000, ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:14:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=20000, ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:15:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=20000, ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 17:16:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run enregistré : max_features=20000, ngram_range=(1, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score,  roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# hyperparamètres à tester\n",
    "param_grid = {\n",
    "    \"max_features\": [5000, 10000, 20000],\n",
    "    \"ngram_range\": [(1, 1), (1, 2), (1, 3)]\n",
    "}\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"TF-IDF + Logistic Regression\")\n",
    "\n",
    "\n",
    "for max_features in param_grid[\"max_features\"]:\n",
    "    for ngram_range in param_grid[\"ngram_range\"]:\n",
    "        # TF-IDF Vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "\n",
    "       \n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "        # régression logistique\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "        \n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # Probabilité positive pour roc_auc\n",
    "       \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        # run MLflow\n",
    "        with mlflow.start_run(run_name=f\"max_features={max_features}, ngram_range={ngram_range}\"):\n",
    "                    mlflow.log_param(\"max_features\", max_features)\n",
    "                    mlflow.log_param(\"ngram_range\", ngram_range)\n",
    "                    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                    mlflow.log_metric(\"f1_score\", f1)\n",
    "                    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "                    #  Génération et enregistrement de la matrice de confusion\n",
    "                    print(f\"\\n Matrice de Confusion pour max_features={max_features}, ngram_range={ngram_range}:\")\n",
    "                    cm_filename = plot_confusion_matrix(y_test, y_pred, \"TF-IDF_GridSearch\", f\"{max_features}_{ngram_range}\")\n",
    "                    mlflow.log_artifact(cm_filename)\n",
    "                    \n",
    "                    mlflow.sklearn.log_model(model, \"logistic_regression_model\")\n",
    "\n",
    "                    print(f\"Run enregistré : max_features={max_features}, ngram_range={ngram_range}, ROC AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.7s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.7s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  11.3s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.7s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  22.8s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  22.6s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=   9.9s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=   9.9s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=   9.9s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  22.7s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  22.6s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  22.5s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=   9.7s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=   9.8s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=   9.8s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  22.6s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  22.1s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  22.6s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.3s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.4s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.5s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  22.7s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  22.4s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  22.6s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=   9.9s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=   9.9s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=   9.9s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  22.8s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  22.7s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  22.7s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=   9.4s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=   9.8s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=   9.8s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  22.6s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  22.2s\n",
      "[CV] END logreg__C=0.01, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  22.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.0s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  11.2s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.8s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  23.4s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  23.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  23.5s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  11.7s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  10.5s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  11.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.1s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.7s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  11.2s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  11.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  10.1s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  24.8s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  24.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  24.8s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.0s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  11.3s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  10.8s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  23.2s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.4s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  23.1s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  11.8s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  10.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  11.5s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  22.9s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.5s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  11.2s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  11.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  10.1s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  24.7s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  24.6s\n",
      "[CV] END logreg__C=0.1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  24.8s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  11.2s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  12.1s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  11.4s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.3s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.6s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.6s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  12.6s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  12.7s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  11.6s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  27.4s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.2s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.3s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  14.5s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  12.9s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  13.3s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  27.9s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  26.5s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  26.2s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  11.2s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  12.1s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  11.5s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.4s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.6s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.5s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  12.6s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  12.8s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  11.7s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  27.3s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.2s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  23.2s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  14.5s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  13.0s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  13.3s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  27.9s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  26.7s\n",
      "[CV] END logreg__C=1, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  26.3s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  12.0s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  12.1s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  12.0s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.4s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.6s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  23.7s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  13.3s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  15.2s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  11.1s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  25.9s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  25.7s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  25.9s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  12.7s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  13.8s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  14.4s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  26.9s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  27.9s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.95, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  28.4s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  11.8s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  12.1s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 1); total time=  12.0s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.4s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  24.6s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=5000, tfidf__ngram_range=(1, 2); total time=  23.9s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  13.3s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  15.2s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 1); total time=  11.1s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  25.8s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  25.8s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=10000, tfidf__ngram_range=(1, 2); total time=  26.2s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  12.7s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  13.7s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 1); total time=  14.4s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  27.0s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  27.9s\n",
      "[CV] END logreg__C=10, tfidf__max_df=0.9, tfidf__max_features=20000, tfidf__ngram_range=(1, 2); total time=  28.3s\n",
      "Run 1 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.95, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 2 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.95, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 3 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.95, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 4 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.95, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 5 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 6 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 7 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 8 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 9 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 10 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 11 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.9, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 12 enregistré avec les hyperparamètres : {'logreg__C': 0.01, 'tfidf__max_df': 0.9, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 13 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 14 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 15 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 16 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 17 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 18 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 19 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 20 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 21 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 22 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 23 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 24 enregistré avec les hyperparamètres : {'logreg__C': 0.1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 25 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 26 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 27 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 28 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 29 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 30 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 31 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 32 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 33 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 34 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 35 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 36 enregistré avec les hyperparamètres : {'logreg__C': 1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 37 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.95, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 38 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.95, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 39 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.95, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 40 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.95, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 41 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 42 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 43 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 44 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 45 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 46 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 47 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.9, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n",
      "Run 48 enregistré avec les hyperparamètres : {'logreg__C': 10, 'tfidf__max_df': 0.9, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Accuracy: 0.78900625, F1 Score: 0.7939023571284669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "\n",
    "# pipeline TF-IDF et Régression Logistique\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000, 20000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__max_df': [0.95, 0.90],\n",
    "    'logreg__C': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"GridSearch TF-IDF + Logistic Regression\")\n",
    "\n",
    "# GridSearch\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='roc_auc', verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "for i, params in enumerate(grid_search.cv_results_['params']):\n",
    "    with mlflow.start_run(run_name=f\"Run_{i+1}\"):\n",
    "        # enregistrer les hyperparamètres\n",
    "        mlflow.log_param(\"max_features\", params['tfidf__max_features'])\n",
    "        mlflow.log_param(\"ngram_range\", params['tfidf__ngram_range'])\n",
    "        mlflow.log_param(\"max_df\", params['tfidf__max_df'])\n",
    "        mlflow.log_param(\"C\", params['logreg__C'])\n",
    "\n",
    "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "        y_prob = grid_search.best_estimator_.predict_proba(X_test)[:, 1]  # Probabilité positive pour roc_auc\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        \n",
    "        # Enregistrer dans MLflow\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "        #  Générer et enregistrer la matrice de confusion\n",
    "        print(f\"\\n Matrice de Confusion pour Run {i+1} :\")\n",
    "        cm_filename = plot_confusion_matrix(y_test, y_pred, \"GridSearch\", i+1)\n",
    "        mlflow.log_artifact(cm_filename)\n",
    "\n",
    "        print(f\"Run {i+1} enregistré avec les hyperparamètres : {params}\")\n",
    "        print(f\"Accuracy: {accuracy}, F1 Score: {f1}, ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "# Entraîner les modèles Word2Vec et FastText \n",
    "w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=2, workers=4)\n",
    "ft_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=2, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def vectorize_text(tokens, model):\n",
    "    vector_size = model.vector_size\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    \n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Transformer les tweets avec les embeddings\n",
    "X_train_w2v = np.array([vectorize_text(tweet, w2v_model) for tweet in X_train])\n",
    "X_test_w2v = np.array([vectorize_text(tweet, w2v_model) for tweet in X_test])\n",
    "\n",
    "X_train_ft = np.array([vectorize_text(tweet, ft_model) for tweet in X_train])\n",
    "X_test_ft = np.array([vectorize_text(tweet, ft_model) for tweet in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_w2v = scaler.fit_transform(X_train_w2v)\n",
    "X_test_w2v = scaler.transform(X_test_w2v)\n",
    "\n",
    "X_train_ft = scaler.fit_transform(X_train_ft)\n",
    "X_test_ft = scaler.transform(X_test_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(X_train, X_test, y_train, y_test, model_name=\"Word2Vec\"):\n",
    "    with mlflow.start_run(run_name=f\"LogisticRegression_{model_name}\"):\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        mlflow.log_param(\"embedding\", model_name)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "\n",
    "        # Afficher la matrice de confusion dans le notebook\n",
    "        print(f\"\\n Matrice de Confusion pour Run {i+1} :\")\n",
    "        cm_filename = plot_confusion_matrix(y_test, y_pred, \"GridSearch\", i+1)\n",
    "        mlflow.log_artifact(cm_filename)\n",
    "\n",
    "        mlflow.sklearn.log_model(model, f\"LogisticRegression_{model_name}\")\n",
    "\n",
    "        print(f\"{model_name} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "train_and_log_model(X_train_w2v, X_test_w2v, y_train, y_test, model_name=\"Word2Vec\")\n",
    "train_and_log_model(X_train_ft, X_test_ft, y_train, y_test, model_name=\"FastText\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 modeles deeplearning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Paramètres\n",
    "MAX_NB_WORDS = 20000  # Nombre maximal de mots dans le vocabulaire\n",
    "MAX_SEQUENCE_LENGTH = 50  # Longueur maximale des séquences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data[\"text\"]) \n",
    "\n",
    "# Conversion  tokens en indices\n",
    "X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "# Conversion  array\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"Données préparées pour LSTM avec padding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "\n",
    "def create_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=MAX_NB_WORDS, output_dim=128, input_length=MAX_SEQUENCE_LENGTH),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lstm_model = create_lstm_model()\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import numpy as np\n",
    "\n",
    "with mlflow.start_run(run_name=\"LSTM_Model\"):\n",
    "    history = lstm_model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_test_pad, y_test),\n",
    "        epochs=5,\n",
    "        batch_size=64,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Prédictions\n",
    "    y_pred = (lstm_model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "    y_prob = lstm_model.predict(X_test_pad)[:, 0]  # Probabilité pour le ROC AUC\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    # Enregistrer les métriques demandées dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "    # Générer et enregistrer la matrice de confusion pour LSTM\n",
    "    cm_filename = plot_confusion_matrix(y_test, y_pred, \"LSTM\", \"1\")\n",
    "    mlflow.log_artifact(cm_filename)\n",
    "\n",
    "    print(f\"LSTM Model enregistré : Accuracy={accuracy:.4f}, F1={f1:.4f}, ROC AUC={roc_auc:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
